#!/usr/bin/env python3
"""
é›†æˆç½‘é¡µæ–‡æœ¬ç›‘æ§äº¤æ˜“ç³»ç»Ÿ
æ•´åˆæ–‡æœ¬ç›‘æ§å’Œå¯Œé€”äº¤æ˜“åŠŸèƒ½
"""

import os
import json
import time
import logging
import warnings
from datetime import datetime
import pandas as pd

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from web_text_parser import find_latest_text_file, parse_unusual_whales_text, extract_ticker_premium_only
from text_compare import has_significant_change, quick_content_check
from futu_strategy_executor import FutuStrategyExecutor

warnings.filterwarnings('ignore')

# é…ç½®å‚æ•°
WEB_TEXT_DIR = "/Users/zhanglijia/Documents/WebText"
DIFFERENCE_THRESHOLD = 0.001
JSON_STORAGE_DIR = "json_history"
EXTRACTED_JSON_DIR = "extracted_jsons"
CURRENT_TRADES_DIR = "current_trades"
BASELINE_DIR = "baseline_data"
CHECK_INTERVAL = 20

def get_current_json_filename():
    """è·å–å½“å‰æ—¥æœŸçš„JSONæ–‡ä»¶å"""
    today = datetime.now().strftime("%Y%m%d")
    return os.path.join(CURRENT_TRADES_DIR, f"current_trades_{today}.json")

def get_baseline_json_filename():
    """è·å–åŸºçº¿æ•°æ®çš„JSONæ–‡ä»¶å"""
    today = datetime.now().strftime("%Y%m%d")
    return os.path.join(BASELINE_DIR, f"baseline_{today}.json")

PROCESSED_FILES_FILE = "processed_files.json"

# é…ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®ï¼ŒæŒ‰æ—¥æœŸä¿å­˜"""
    today = datetime.now().strftime("%Y%m%d")
    log_filename = f"logs/integrated_web_trader_{today}.log"
    
    # åˆ›å»ºlogsç›®å½•
    os.makedirs("logs", exist_ok=True)
    
    # æ¸…é™¤ç°æœ‰handlers
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # åˆ›å»ºformatter
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # æ–‡ä»¶handler - å¯ç”¨ç«‹å³åˆ·æ–°
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setFormatter(formatter)
    file_handler.flush = lambda: file_handler.stream.flush()  # å¼ºåˆ¶åˆ·æ–°
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # é…ç½®æ ¹logger
    logging.basicConfig(
        level=logging.INFO,
        handlers=[file_handler, console_handler],
        force=True
    )
    
    # è·å–loggerå¹¶è®¾ç½®è‡ªåŠ¨åˆ·æ–°
    logger = logging.getLogger(__name__)
    
    # é‡å†™loggeræ–¹æ³•ä»¥è‡ªåŠ¨åˆ·æ–°
    original_info = logger.info
    original_error = logger.error
    original_warning = logger.warning
    
    def info_with_flush(*args, **kwargs):
        original_info(*args, **kwargs)
        file_handler.flush()
    
    def error_with_flush(*args, **kwargs):
        original_error(*args, **kwargs)
        file_handler.flush()
    
    def warning_with_flush(*args, **kwargs):
        original_warning(*args, **kwargs)
        file_handler.flush()
    
    logger.info = info_with_flush
    logger.error = error_with_flush
    logger.warning = warning_with_flush
    
    return logger

logger = setup_logging()

class IntegratedWebTrader:
    def __init__(self, initial_capital: float = 100000):
        """åˆå§‹åŒ–é›†æˆç½‘é¡µæ–‡æœ¬ç›‘æ§äº¤æ˜“ç³»ç»Ÿ"""
        # åˆå§‹åŒ–FutuStrategyExecutor
        self.futu_executor = FutuStrategyExecutor(initial_capital=initial_capital)
        
        # æ–‡ä»¶å¤„ç†çŠ¶æ€
        self.processed_files = self.load_processed_files()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(JSON_STORAGE_DIR, exist_ok=True)
        os.makedirs(EXTRACTED_JSON_DIR, exist_ok=True)
        os.makedirs(CURRENT_TRADES_DIR, exist_ok=True)
        os.makedirs(BASELINE_DIR, exist_ok=True)
    
    def load_processed_files(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        if os.path.exists(PROCESSED_FILES_FILE):
            with open(PROCESSED_FILES_FILE, 'r') as f:
                return json.load(f)
        return {"processed": [], "last_file": None, "last_hash": None}
    
    def save_processed_files(self):
        """ä¿å­˜å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        with open(PROCESSED_FILES_FILE, 'w') as f:
            json.dump(self.processed_files, f, indent=2)
    
    def get_unprocessed_files(self):
        """è·å–æœªå¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        all_files = [f for f in os.listdir(WEB_TEXT_DIR) if f.endswith('.txt')]
        processed_set = set(self.processed_files["processed"])
        unprocessed = [f for f in all_files if f not in processed_set]
        unprocessed.sort(key=lambda x: os.path.getmtime(os.path.join(WEB_TEXT_DIR, x)))
        return unprocessed
    
    def compare_json_data(self, baseline_data, new_data):
        """æ¯”è¾ƒæ•°æ®ï¼Œæ‰¾å‡ºæ–°å¢äº¤æ˜“"""
        baseline_trades = {f"{t['ticker']}_{t['premium']}" for t in baseline_data["trades"]}
        new_trades = {f"{t['ticker']}_{t['premium']}" for t in new_data["trades"]}
        added_keys = new_trades - baseline_trades
        
        new_trade_items = []
        for trade in new_data["trades"]:
            if f"{trade['ticker']}_{trade['premium']}" in added_keys:
                new_trade_items.append(trade)
        
        return {"new_trades": new_trade_items}
    
    def create_trades_csv(self, new_trades_data, timestamp):
        """åˆ›å»ºäº¤æ˜“CSVæ–‡ä»¶"""
        trades_list = []
        for trade in new_trades_data["new_trades"]:
            trades_list.append({
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '15:30:00',
                'underlying_symbol': trade['ticker'],
                'premium': trade['premium'],
                'underlying_price': 100.0
            })
        
        df = pd.DataFrame(trades_list)
        csv_path = os.path.join("real_time_option", f"trades_{timestamp}.csv")
        os.makedirs("real_time_option", exist_ok=True)
        df.to_csv(csv_path, index=False)
        
        print(f"ğŸ’¾ äº¤æ˜“CSVæ–‡ä»¶å·²åˆ›å»º: {csv_path}")
        logger.info(f"äº¤æ˜“CSVæ–‡ä»¶å·²åˆ›å»º: {csv_path}")
        return csv_path
    
    def execute_new_trades(self, new_trades_data, timestamp):
        """æ‰§è¡Œæ–°å¢äº¤æ˜“å¹¶è®°å½•ç»“æœ"""
        new_trades = new_trades_data["new_trades"]
        print(f"\nğŸ“ˆ å¼€å§‹æ‰§è¡Œ {len(new_trades)} ç¬”æ–°å¢äº¤æ˜“")
        logger.info(f"å¼€å§‹æ‰§è¡Œ {len(new_trades)} ç¬”æ–°å¢äº¤æ˜“")
        
        self.create_trades_csv(new_trades_data, timestamp)
        
        print("ğŸ“Š æ–°å¢äº¤æ˜“è¯¦æƒ…:")
        for trade in new_trades:
            print(f"  ğŸ“ {trade['ticker']}: {trade['premium']:,}")
        
        execution_results = []
        
        for i, trade in enumerate(new_trades, 1):
            ticker = trade['ticker']
            premium = trade['premium']
            
            print(f"\nğŸ“‹ [{i}/{len(new_trades)}] å¤„ç†äº¤æ˜“: {ticker} - Premium: {premium:,}")
            logger.info(f"[{i}/{len(new_trades)}] å¤„ç†äº¤æ˜“: {ticker} - Premium: {premium:,}")
            
            single_trade_df = pd.DataFrame([{
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '15:30:00',
                'underlying_symbol': ticker,
                'premium': premium,
                'underlying_price': 100.0
            }])
            
            trade_result = {
                "ticker": ticker,
                "premium": premium,
                "timestamp": timestamp,
                "status": "unknown",
                "message": ""
            }
            
            old_orders_count = len([p for p in self.futu_executor.positions.values() if p.get('order_id') != 'PENDING'])
            
            self.futu_executor.process_option_signal(single_trade_df.iloc[0])
            new_orders_count = len([p for p in self.futu_executor.positions.values() if p.get('order_id') != 'PENDING'])
            
            if new_orders_count > old_orders_count:
                trade_result["status"] = "executed"
                trade_result["message"] = "äº¤æ˜“å·²æ‰§è¡Œ"
                print(f"âœ… äº¤æ˜“å·²æ‰§è¡Œ: {ticker}")
                logger.info(f"äº¤æ˜“å·²æ‰§è¡Œ: {ticker}")
            else:
                trade_result["status"] = "skipped"
                trade_result["message"] = "äº¤æ˜“è¢«è·³è¿‡"
                print(f"â­ï¸  äº¤æ˜“è¢«è·³è¿‡: {ticker}")
                logger.info(f"äº¤æ˜“è¢«è·³è¿‡: {ticker}")
            
            execution_results.append(trade_result)
        
        self.update_current_trades_file(execution_results, timestamp)
    
    def update_current_trades_file(self, execution_results, timestamp):
        """æ›´æ–°å½“å‰äº¤æ˜“æ–‡ä»¶"""
        current_json_file = get_current_json_filename()
        
        existing_trades = []
        if os.path.exists(current_json_file):
            with open(current_json_file, 'r', encoding='utf-8') as f:
                existing_trades = json.load(f)["trades"]
        
        existing_keys = {f"{t['ticker']}_{t['premium']}_{t['timestamp']}" for t in existing_trades}
        
        new_trades_added = 0
        for result in execution_results:
            trade_key = f"{result['ticker']}_{result['premium']}_{result['timestamp']}"
            if trade_key not in existing_keys:
                existing_trades.append(result)
                new_trades_added += 1
        
        updated_data = {
            "timestamp": timestamp,
            "last_update": datetime.now().isoformat(),
            "total_trades": len(existing_trades),
            "trades": existing_trades
        }
        
        with open(current_json_file, 'w', encoding='utf-8') as f:
            json.dump(updated_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ äº¤æ˜“è®°å½•å·²æ›´æ–°: {current_json_file} (æ–°å¢ {new_trades_added} ç¬”ï¼Œç´¯ç§¯ {len(existing_trades)} ç¬”)")
        logger.info(f"äº¤æ˜“è®°å½•å·²æ›´æ–°: {current_json_file} (æ–°å¢ {new_trades_added} ç¬”ï¼Œç´¯ç§¯ {len(existing_trades)} ç¬”)")
    
    def process_single_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        filename = os.path.basename(file_path)
        import re
        timestamp_match = re.search(r'(\d{8}_\d{6})', filename)
        timestamp = timestamp_match.group(1) if timestamp_match else datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print(f"\nå¤„ç†æ–‡ä»¶: {filename}")
        print(f"æ—¶é—´æˆ³: {timestamp}")
        
        extracted_data = parse_unusual_whales_text(file_path)
        simplified_data = extract_ticker_premium_only(extracted_data)
        
        print(f"âœ… è§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(simplified_data['trades'])} æ¡äº¤æ˜“")
        
        extracted_json_path = os.path.join(EXTRACTED_JSON_DIR, f"extracted_{timestamp}.json")
        with open(extracted_json_path, 'w', encoding='utf-8') as f:
            json.dump({"timestamp": timestamp, "source_file": filename, "extracted_data": extracted_data}, 
                     f, indent=2, ensure_ascii=False)
        print(f"æå–JSONå·²ä¿å­˜åˆ°: {extracted_json_path}")
        
        baseline_json_file = get_baseline_json_filename()
        
        if not os.path.exists(baseline_json_file):
            print("ğŸ“ è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ›å»ºåŸºçº¿æ•°æ®...")
            
            with open(baseline_json_file, 'w', encoding='utf-8') as f:
                json.dump(simplified_data, f, indent=2, ensure_ascii=False)
            print(f"åŸºçº¿æ•°æ®å·²ä¿å­˜: {baseline_json_file}")
            
            print(f"\nğŸ“Š åŸºçº¿äº¤æ˜“åˆ—è¡¨:")
            for i, trade in enumerate(simplified_data['trades'], 1):
                print(f"  {i}. {trade['ticker']}: {trade['premium']:,}")
            
            print(f"\nâ„¹ï¸  åˆå§‹çŠ¶æ€ä¸æ‰§è¡Œäº¤æ˜“ï¼Œä»…å»ºç«‹åŸºçº¿æ•°æ®")
            print(f"\nâœ… åŸºçº¿å·²å»ºç«‹ï¼ŒåŒ…å« {len(simplified_data['trades'])} æ¡äº¤æ˜“")
        else:
            latest_extracted = None
            extracted_files = [f for f in os.listdir(EXTRACTED_JSON_DIR) if f.startswith('extracted_') and f.endswith('.json')]
            extracted_files.sort()
            
            if len(extracted_files) >= 2:
                latest_extracted = os.path.join(EXTRACTED_JSON_DIR, extracted_files[-2])
            
            if latest_extracted:
                with open(latest_extracted, 'r', encoding='utf-8') as f:
                    previous_full_data = json.load(f)
                previous_simplified = extract_ticker_premium_only(previous_full_data["extracted_data"])
                print(f"ğŸ“‹ ä¸ä¸Šä¸€æ¬¡æ•°æ®æ¯”è¾ƒ: {os.path.basename(latest_extracted)}")
            else:
                with open(baseline_json_file, 'r', encoding='utf-8') as f:
                    previous_simplified = json.load(f)
                print(f"ğŸ“‹ ä¸åŸºçº¿æ•°æ®æ¯”è¾ƒ")
            
            differences = self.compare_json_data(previous_simplified, simplified_data)
            
            if differences["new_trades"]:
                print(f"ğŸ”„ å‘ç° {len(differences['new_trades'])} ç¬”æ–°å¢äº¤æ˜“")
                
                diff_json_path = os.path.join(JSON_STORAGE_DIR, f"difference_{timestamp}.json")
                with open(diff_json_path, 'w', encoding='utf-8') as f:
                    json.dump({"timestamp": timestamp, **differences}, f, indent=2, ensure_ascii=False)
                print(f"å·®å¼‚JSONå·²ä¿å­˜åˆ°: {diff_json_path}")
                
                self.execute_new_trades(differences, timestamp)
            else:
                print("â„¹ï¸  æ²¡æœ‰å‘ç°æ–°å¢äº¤æ˜“")
        
        self.processed_files["processed"].append(filename)
        self.processed_files["last_file"] = filename
        self.save_processed_files()
    
    def run(self):
        """è¿è¡Œç›‘æ§ç³»ç»Ÿ"""
        logger.info("é›†æˆç½‘é¡µæ–‡æœ¬ç›‘æ§äº¤æ˜“ç³»ç»Ÿå¯åŠ¨...")
        logger.info(f"ç›‘æ§ç›®å½•: {WEB_TEXT_DIR}")
        logger.info(f"æ£€æŸ¥é—´éš”: {CHECK_INTERVAL}ç§’")
        
        while True:
            unprocessed_files = self.get_unprocessed_files()
            
            if unprocessed_files:
                print(f"\nå‘ç° {len(unprocessed_files)} ä¸ªæœªå¤„ç†çš„æ–‡ä»¶")
                for file_name in unprocessed_files:
                    file_path = os.path.join(WEB_TEXT_DIR, file_name)
                    self.process_single_file(file_path)
            else:
                print(f"æ²¡æœ‰æ–°æ–‡ä»¶ï¼Œç­‰å¾…{CHECK_INTERVAL}ç§’...")
            
            time.sleep(CHECK_INTERVAL)

def main():
    """ä¸»å…¥å£"""
    trader = IntegratedWebTrader(initial_capital=100000)
    trader.run()

if __name__ == "__main__":
    main()